{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# –ò–º–ø–æ—Ä—Ç –ë–∏–±–ª–∏–æ—Ç–µ–∫ –¥–ª—è —Ä–∞–±–æ—Ç—ã"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zR17f5njr2Bq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫ –¥–ª—è –¥–∞–Ω–Ω—ã—Ö –∏ –æ—Å–Ω–æ–≤–Ω—ã–µ –ø—É—Ç–∏"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', 100)\n",
        "\n",
        "TRAIN_DATA_PATH = \"train.csv\"\n",
        "TEST_DATA_PATH = \"test.csv\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGwcHqOYriqh"
      },
      "source": [
        "# –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "941ihwUEdetf"
      },
      "outputs": [],
      "source": [
        "class DataNormalizer:\n",
        "    \"\"\"\n",
        "    –ö–ª–∞—Å—Å –¥–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏–∏ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.\n",
        "    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –∫ –Ω—É–ª–µ–≤–æ–º—É —Å—Ä–µ–¥–Ω–µ–º—É –∏ –µ–¥–∏–Ω–∏—á–Ω–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–∏.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.mean_values = None\n",
        "        self.std_values = None\n",
        "\n",
        "    def calculate_statistics(self, data_matrix):\n",
        "        self.mean_values = np.mean(data_matrix, axis=0)\n",
        "        self.std_values = np.std(data_matrix, axis=0)\n",
        "        # –ó–∞—â–∏—Ç–∞ –æ—Ç –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å\n",
        "        self.std_values[self.std_values == 0] = 1.0\n",
        "        return self\n",
        "    def apply_normalization(self, data_matrix):\n",
        "        return (data_matrix - self.mean_values) / self.std_values\n",
        "    \n",
        "    def fit_and_transform(self, data_matrix):\n",
        "        self.calculate_statistics(data_matrix)\n",
        "        return self.apply_normalization(data_matrix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPzxI7IRskie"
      },
      "source": [
        "# –ª–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e21hodG6sj9O"
      },
      "outputs": [],
      "source": [
        "class RegularizedLinearModel:\n",
        "    \"\"\"\n",
        "    –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –ª–∏–Ω–µ–π–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ —Å L2-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π (Ridge).\n",
        "    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –±–æ—Ä—å–±—ã —Å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ–º.\n",
        "    \"\"\"\n",
        "    def __init__(self, regularization_strength=1.0):\n",
        "        self.reg_param = regularization_strength\n",
        "        self.feature_weights = None\n",
        "        self.constant_term = None\n",
        "    \n",
        "    def train_model(self, feature_matrix, target_vector):\n",
        "        # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–æ–ª–±–µ—Ü –¥–ª—è —Å–≤–æ–±–æ–¥–Ω–æ–≥–æ —á–ª–µ–Ω–∞\n",
        "        design_matrix = np.c_[np.ones((feature_matrix.shape[0], 1)), feature_matrix]\n",
        "        num_parameters = design_matrix.shape[1]\n",
        "        identity_matrix = np.eye(num_parameters)\n",
        "        identity_matrix[0, 0] = 0  # –ù–µ —Ä–µ–≥—É–ª—è—Ä–∏–∑—É–µ–º —Å–≤–æ–±–æ–¥–Ω—ã–π —á–ª–µ–Ω\n",
        "        \n",
        "        # –†–µ—à–µ–Ω–∏–µ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —É—Ä–∞–≤–Ω–µ–Ω–∏—è —Å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π\n",
        "        XtX = design_matrix.T.dot(design_matrix)\n",
        "        Xty = design_matrix.T.dot(target_vector)\n",
        "        regularized_XtX = XtX + self.reg_param * identity_matrix\n",
        "        \n",
        "        try:\n",
        "            self.model_parameters = np.linalg.solve(regularized_XtX, Xty)\n",
        "        except np.linalg.LinAlgError:\n",
        "            self.model_parameters = np.linalg.pinv(regularized_XtX).dot(Xty)\n",
        "            \n",
        "        self.constant_term = self.model_parameters[0]\n",
        "        self.feature_weights = self.model_parameters[1:]\n",
        "        \n",
        "    def generate_predictions(self, feature_matrix):\n",
        "        return feature_matrix.dot(self.feature_weights) + self.constant_term\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGkms08FtBTi"
      },
      "source": [
        "# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wj9XiqRytCrH"
      },
      "outputs": [],
      "source": [
        "def scale_to_unit_range(series):\n",
        "    \"\"\"\n",
        "    –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∞ –∫ –¥–∏–∞–ø–∞–∑–æ–Ω—É [0, 1].\n",
        "    \"\"\"\n",
        "    return (series - series.min()) / (series.max() - series.min())\n",
        "\n",
        "def process_temporal_features(dataframe):\n",
        "    \"\"\"\n",
        "    –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫.\n",
        "    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –¥–∞—Ç—É –≤ —á–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏.\n",
        "    \"\"\"\n",
        "    df_processed = dataframe.copy()\n",
        "    if 'ApplicationDate' in df_processed.columns:\n",
        "        df_processed['ApplicationDate'] = pd.to_datetime(df_processed['ApplicationDate'], errors='coerce')\n",
        "        \n",
        "        most_common = df_processed['ApplicationDate'].mode()[0]\n",
        "        df_processed['ApplicationDate'] = df_processed['ApplicationDate'].fillna(most_common)\n",
        "        \n",
        "        reference_date = pd.Timestamp(\"1970-01-01\")\n",
        "        df_processed[\"DaysSinceReference\"] = (df_processed[\"ApplicationDate\"] - reference_date).dt.days\n",
        "        df_processed[\"YearOfApplication\"] = df_processed[\"ApplicationDate\"].dt.year\n",
        "        df_processed[\"MonthOfApplication\"] = df_processed[\"ApplicationDate\"].dt.month\n",
        "        \n",
        "        df_processed = df_processed.drop(columns=['ApplicationDate'])\n",
        "    return df_processed\n",
        "\n",
        "def calculate_geometric_metrics(dataframe):\n",
        "    \"\"\"\n",
        "    –°–æ–∑–¥–∞–Ω–∏–µ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç—Ä–∏–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π.\n",
        "    \"\"\"\n",
        "    df_processed = dataframe.copy()\n",
        "    \n",
        "    credit_score = df_processed['CreditScore'].fillna(df_processed['CreditScore'].median())\n",
        "    annual_income = df_processed['AnnualIncome'].fillna(df_processed['AnnualIncome'].median())\n",
        "    debt_ratio = df_processed['DebtToIncomeRatio'].fillna(df_processed['DebtToIncomeRatio'].median())\n",
        "    \n",
        "    norm_score = scale_to_unit_range(credit_score)\n",
        "    norm_income = scale_to_unit_range(annual_income)\n",
        "    norm_debt = scale_to_unit_range(debt_ratio)\n",
        "    \n",
        "    df_processed['DistanceToRisk'] = np.sqrt(norm_score**2 + norm_income**2 + (1 - norm_debt)**2)\n",
        "    df_processed['DistanceToStability'] = np.sqrt((1 - norm_score)**2 + (1 - norm_income)**2 + norm_debt**2)\n",
        "    \n",
        "    return df_processed\n",
        "\n",
        "def engineer_base_features(dataframe):\n",
        "    \"\"\"\n",
        "    –ë–∞–∑–æ–≤–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤,\n",
        "    –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—Ä–æ–≤–∞–Ω–∏–µ, —Å–æ–∑–¥–∞–Ω–∏–µ –æ—Ç–Ω–æ—à–µ–Ω–∏–π –∏ –±–∏–Ω–∏–Ω–≥.\n",
        "    \"\"\"\n",
        "    df_processed = dataframe.copy()\n",
        "    \n",
        "    # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π\n",
        "    numeric_columns = df_processed.select_dtypes(include=[np.number]).columns\n",
        "    for column in numeric_columns:\n",
        "        df_processed[column] = df_processed[column].fillna(df_processed[column].median())\n",
        "        \n",
        "    # –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∏–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è\n",
        "    columns_for_log = [\"AnnualIncome\", \"TotalAssets\", \"TotalLiabilities\", \"NetWorth\", \"LoanAmount\"]\n",
        "    for column in columns_for_log:\n",
        "        if column in df_processed.columns:\n",
        "            minimum_value = df_processed[column].min()\n",
        "            offset = abs(minimum_value) + 1.0 if minimum_value < 0 else 1.0\n",
        "            df_processed[f'LogTransformed_{column}'] = np.log(df_processed[column] + offset)\n",
        "\n",
        "    # –û—Ç–Ω–æ—à–µ–Ω–∏—è –º–µ–∂–¥—É –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏\n",
        "    if 'AnnualIncome' in df_processed.columns and 'NumberOfDependents' in df_processed.columns:\n",
        "        df_processed['IncomePerMember'] = df_processed['AnnualIncome'] / (df_processed['NumberOfDependents'] + 1)\n",
        "        \n",
        "    if 'TotalLiabilities' in df_processed.columns and 'AnnualIncome' in df_processed.columns:\n",
        "        df_processed['LiabilitiesToIncome'] = df_processed['TotalLiabilities'] / (df_processed['AnnualIncome'] + 10)\n",
        "        \n",
        "    if 'InterestRate' in df_processed.columns and 'BaseInterestRate' in df_processed.columns:\n",
        "        df_processed['InterestDifference'] = df_processed['InterestRate'] - df_processed['BaseInterestRate']\n",
        "\n",
        "    # –î–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "    if 'CreditScore' in df_processed.columns:\n",
        "        df_processed['CreditScoreCategory'] = pd.qcut(df_processed['CreditScore'], q=10, labels=False, duplicates='drop')\n",
        "    if 'AnnualIncome' in df_processed.columns:\n",
        "        df_processed['IncomeCategory'] = pd.qcut(df_processed['AnnualIncome'], q=5, labels=False, duplicates='drop')\n",
        "    if 'LoanAmount' in df_processed.columns:\n",
        "        df_processed['LoanAmountCategory'] = pd.qcut(df_processed['LoanAmount'], q=10, labels=False, duplicates='drop')\n",
        "    if 'DebtToIncomeRatio' in df_processed.columns:\n",
        "        df_processed['DTICategory'] = pd.qcut(df_processed['DebtToIncomeRatio'], q=10, labels=False, duplicates='drop')\n",
        "        \n",
        "    return df_processed\n",
        "\n",
        "def prepare_features_for_model(training_data, testing_data):\n",
        "    \"\"\"\n",
        "    –û—Å–Ω–æ–≤–Ω–æ–π –ø–∞–π–ø–ª–∞–π–Ω –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö:\n",
        "    1. –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "    2. –°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –∏ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "    3. Target Encoding –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö\n",
        "    4. –ü–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω—ã–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è\n",
        "    \"\"\"\n",
        "    train_df = training_data.copy()\n",
        "    test_df = testing_data.copy()\n",
        "    \n",
        "    # –£–¥–∞–ª–µ–Ω–∏–µ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π\n",
        "    target_values = train_df['RiskScore']\n",
        "    lower_bound = target_values.quantile(0.012)\n",
        "    upper_bound = target_values.quantile(0.991)\n",
        "    train_df = train_df[(train_df['RiskScore'] >= lower_bound) & (train_df['RiskScore'] <= upper_bound)].reset_index(drop=True)\n",
        "    target_values = train_df['RiskScore']\n",
        "    train_df = train_df.drop('RiskScore', axis=1)\n",
        "    \n",
        "    # –ú–∞—Ä–∫–∏—Ä–æ–≤–∫–∞ –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö\n",
        "    train_df['dataset_type'] = 1\n",
        "    test_df['dataset_type'] = 0\n",
        "    combined_data = pd.concat([train_df, test_df], axis=0).reset_index(drop=True)\n",
        "    combined_data = combined_data.drop(columns=['ID', 'Id'], errors='ignore')\n",
        "    \n",
        "    print(\"   ...–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\")\n",
        "    combined_data = process_temporal_features(combined_data)\n",
        "    \n",
        "    print(\"   ...–°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –∏ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\")\n",
        "    combined_data = engineer_base_features(combined_data)\n",
        "    combined_data = calculate_geometric_metrics(combined_data)\n",
        "    \n",
        "    # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π\n",
        "    categorical_features = [\n",
        "        \"EducationLevel\", \"EmploymentStatus\", \"HomeOwnershipStatus\", \"LoanPurpose\", \n",
        "        \"IncomeCategory\", \"CreditScoreCategory\", \"LoanAmountCategory\", \"DTICategory\"\n",
        "    ]\n",
        "    categorical_features = [feature for feature in categorical_features if feature in combined_data.columns]\n",
        "    \n",
        "    training_subset = combined_data[combined_data['dataset_type'] == 1].copy()\n",
        "    testing_subset = combined_data[combined_data['dataset_type'] == 0].copy()\n",
        "    \n",
        "    training_subset['RiskScore'] = target_values.values\n",
        "    global_median = training_subset['RiskScore'].median()\n",
        "    \n",
        "    print(\"   ...Target Encoding –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\")\n",
        "    for feature in categorical_features:\n",
        "        encoding_map = training_subset.groupby(feature)['RiskScore'].median()\n",
        "        combined_data.loc[combined_data['dataset_type'] == 1, f'{feature}_Encoded'] = training_subset[feature].map(encoding_map)\n",
        "        combined_data.loc[combined_data['dataset_type'] == 0, f'{feature}_Encoded'] = testing_subset[feature].map(encoding_map)\n",
        "        combined_data[f'{feature}_Encoded'] = combined_data[f'{feature}_Encoded'].fillna(global_median)\n",
        "        \n",
        "    # –£–¥–∞–ª–µ–Ω–∏–µ –Ω–µ—á–∏—Å–ª–æ–≤—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤\n",
        "    non_numeric_columns = combined_data.select_dtypes(include=['object', 'category']).columns\n",
        "    combined_data = combined_data.drop(columns=non_numeric_columns, errors='ignore')\n",
        "    \n",
        "    # –í—ã–¥–µ–ª–µ–Ω–∏–µ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "    combined_data = combined_data.select_dtypes(include=[np.number])\n",
        "    \n",
        "    X_train = combined_data[combined_data['dataset_type'] == 1].drop('dataset_type', axis=1).values\n",
        "    X_test = combined_data[combined_data['dataset_type'] == 0].drop('dataset_type', axis=1).values\n",
        "    \n",
        "    # –£–¥–∞–ª–µ–Ω–∏–µ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "    print(\"   ...–£–¥–∞–ª–µ–Ω–∏–µ –º–∞–ª–æ–≤–∞—Ä–∏–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\")\n",
        "    variance_filter = VarianceThreshold(threshold=0)\n",
        "    X_train = variance_filter.fit_transform(X_train)\n",
        "    X_test = variance_filter.transform(X_test)\n",
        "    \n",
        "    # –ü–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω—ã–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "    print(\"   ...–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\")\n",
        "    \n",
        "    feature_correlations = []\n",
        "    for i in range(X_train.shape[1]):\n",
        "        correlation = np.corrcoef(X_train[:, i], target_values)[0, 1]\n",
        "        feature_correlations.append(abs(correlation) if not np.isnan(correlation) else 0)\n",
        "        \n",
        "    top_30_indices = np.argsort(feature_correlations)[-30:]\n",
        "    poly_transformer_2 = PolynomialFeatures(degree=2, include_bias=False)\n",
        "    X_train_poly2 = poly_transformer_2.fit_transform(X_train[:, top_30_indices])[:, 30:]\n",
        "    X_test_poly2 = poly_transformer_2.transform(X_test[:, top_30_indices])[:, 30:]\n",
        "    \n",
        "    top_5_indices = np.argsort(feature_correlations)[-5:]\n",
        "    poly_transformer_3 = PolynomialFeatures(degree=3, include_bias=False)\n",
        "    X_train_poly3 = poly_transformer_3.fit_transform(X_train[:, top_5_indices])[:, 5:]\n",
        "    X_test_poly3 = poly_transformer_3.transform(X_test[:, top_5_indices])[:, 5:]\n",
        "    \n",
        "    X_train_final = np.hstack([X_train, X_train_poly2, X_train_poly3])\n",
        "    X_test_final = np.hstack([X_test, X_test_poly2, X_test_poly3])\n",
        "    \n",
        "    return X_train_final, target_values.values, X_test_final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Zzu0xUnuvGA"
      },
      "source": [
        "# –í–ê–õ–ò–î–ê–¶–ò–Ø –ò –û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ò"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXNxOHy9uxFQ",
        "outputId": "f6e0f634-ecce-4c8d-ebe9-d33684a9c60c"
      },
      "outputs": [],
      "source": [
        "def perform_cross_validation(feature_matrix, target_vector, regularization_param, folds=5):\n",
        "    \"\"\"\n",
        "    –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–ª—è –ø–æ–¥–±–æ—Ä–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏.\n",
        "    \"\"\"\n",
        "    sample_count = len(feature_matrix)\n",
        "    fold_size = sample_count // folds\n",
        "    indices = np.arange(sample_count)\n",
        "    np.random.shuffle(indices)\n",
        "    validation_scores = []\n",
        "    \n",
        "    for fold in range(folds):\n",
        "        val_indices = indices[fold*fold_size : (fold+1)*fold_size]\n",
        "        train_indices = np.setdiff1d(indices, val_indices)\n",
        "        \n",
        "        X_train_fold, X_val_fold = feature_matrix[train_indices], feature_matrix[val_indices]\n",
        "        y_train_fold, y_val_fold = target_vector[train_indices], target_vector[val_indices]\n",
        "        \n",
        "        model = RegularizedLinearModel(regularization_param)\n",
        "        model.train_model(X_train_fold, y_train_fold)\n",
        "        predictions = model.generate_predictions(X_val_fold)\n",
        "        validation_scores.append(np.mean((y_val_fold - predictions) ** 2))\n",
        "        \n",
        "    return np.mean(validation_scores)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# –ó–∞–ø—É—Å–∫"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def execute_analysis():\n",
        "    \"\"\"\n",
        "    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è: –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö, –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤,\n",
        "    –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        training_set = pd.read_csv(TRAIN_DATA_PATH)\n",
        "        testing_set = pd.read_csv(TEST_DATA_PATH)\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–æ–≤: {e}\")\n",
        "        return\n",
        "\n",
        "    print(\"üîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...\")\n",
        "    X_train_processed, y_train_processed, X_test_processed = prepare_features_for_model(training_set, testing_set)\n",
        "    print(f\"‚úÖ –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X_train_processed.shape[1]}\")\n",
        "    \n",
        "    print(\"‚öñÔ∏è –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö...\")\n",
        "    normalizer = DataNormalizer()\n",
        "    X_train_normalized = normalizer.fit_and_transform(X_train_processed)\n",
        "    X_test_normalized = normalizer.apply_normalization(X_test_processed)\n",
        "    \n",
        "    print(\"üîç –ü–æ–¥–±–æ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏...\")\n",
        "    regularization_values = [0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 25.0]\n",
        "    best_cv_score = float('inf')\n",
        "    optimal_parameter = 1.0\n",
        "    \n",
        "    for reg_value in regularization_values:\n",
        "        cv_score = perform_cross_validation(X_train_normalized, y_train_processed, reg_value, folds=5)\n",
        "        print(f\"   –ü–∞—Ä–∞–º–µ—Ç—Ä: {reg_value:<6} | CV MSE: {cv_score:.4f}\")\n",
        "        if cv_score < best_cv_score:\n",
        "            best_cv_score = cv_score\n",
        "            optimal_parameter = reg_value\n",
        "            \n",
        "    print(f\"üèÜ –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä: {optimal_parameter}\")\n",
        "    \n",
        "    final_model = RegularizedLinearModel(optimal_parameter)\n",
        "    final_model.train_model(X_train_normalized, y_train_processed)\n",
        "    \n",
        "    train_predictions = final_model.generate_predictions(X_train_normalized)\n",
        "    print(f\"üìä MSE –Ω–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ: {np.mean((y_train_processed - train_predictions) ** 2):.4f}\")\n",
        "    \n",
        "    test_predictions = final_model.generate_predictions(X_test_normalized)\n",
        "    # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n",
        "    test_predictions = np.clip(test_predictions, 0, 100)\n",
        "    \n",
        "    result_dataframe = pd.DataFrame({\n",
        "        'ID': testing_set['ID'] if 'ID' in testing_set.columns else range(len(test_predictions)),\n",
        "        'RiskScore': test_predictions\n",
        "    })\n",
        "    result_dataframe.to_csv('prediction_results.csv', index=False)\n",
        "    print(\"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: prediction_results.csv\")\n",
        "    \n",
        "    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n",
        "    sns.histplot(result_dataframe[\"RiskScore\"], bins=30)\n",
        "    plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π RiskScore')\n",
        "    plt.xlabel('RiskScore')\n",
        "    plt.ylabel('–ß–∞—Å—Ç–æ—Ç–∞')\n",
        "    plt.show()\n",
        "\n",
        "    if __name__ == \"__main__\":\n",
        "        execute_analysis()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
